{"0": {
    "doc": "Quick start tutorial",
    "title": "Quick start tutorial",
    "content": "# Quick start tutorial Use your command-line interface to **connect** Soda SQL to your database, prepare default **tests** that will surface \"bad\" data, then run your first **scan** in a few minutes. ![tutorial-happy-path](../assets/images/tutorial-happy-path.png){:height=\"600px\" width=\"600px\"} ## Create a sample warehouse (optional) In the context of Soda SQL, a warehouse represents a SQL engine or database such as Snowflake, AWS Redshift, or PostgreSQL. If you do not have access to a warehouse on your system, you can use Docker to build a sample PostgreSQL warehouse so that you can set up your Soda SQL CLI tool and see it in action. All the instructions below reference this sample warehouse in the commands. 1. From your command-line interface, execute the following to build a containerized PostgreSQL warehouse. Note that the `-v` option connects to a location on your local drive in which Soda SQL will create your warehouse directory file further in this tutorial. ```shell $ docker run --name soda_sql_tutorial_db --rm -d \\ -p 5432:5432 \\ -v soda_sql_tutorial_postgres:/var/lib/postgresql/data:rw \\ -e POSTGRES_USER=sodasql \\ -e POSTGRES_DB=sodasql \\ -e POSTGRES_HOST_AUTH_METHOD=trust \\ postgres:9.6.17-alpine ``` 2. Load sample data into your warehouse. ```shell docker exec soda_sql_tutorial_db \\ sh -c \"wget -qO - https://raw.githubusercontent.com/sodadata/soda-sql/main/tests/demo/demodata.sql | psql -U sodasql -d sodasql\" ``` ## Connect Soda SQL to the warehouse The instructions below reference the sample warehouse in the commands. Customize the example commands to use your own PostgreSQL warehouse connection details, if you like. 1. From your command-line interface, verify your [installation]({% link getting-started/installation.md %}) of Soda SQL using the `soda` command. ```shell $ soda Usage: soda [OPTIONS] COMMAND [ARGS]... ``` 2. Create, then navigate to a new Soda SQL warehouse directory. The example below creates a directory named `soda-sql-tutorial`. ```shell $ mkdir soda_sql_tutorial $ cd soda_sql_tutorial ``` 3. Use the `soda create` command to create and pre-populate two files that enable you to configure connection details for Soda SQL to access your warehouse: * a `warehouse.yml` file which stores access details for your warehouse ([read more]({% link documentation/warehouse.md %})) * an `env_vars.yml` file which securely stores warehouse login credentials ([read more]({% link documentation/warehouse.md %}#env_vars-yaml)) Command: ```shell $ soda create -d sodasql -u sodasql -w soda_sql_tutorial postgres ``` Output: ```shell | Soda CLI version ... | Creating warehouse YAML file warehouse.yml ... | Creating /Users/tom/.soda/env_vars.yml with example env vars in section soda_sql_tutorial | Review warehouse.yml by running command | cat warehouse.yml | Review section soda_sql_tutorial in ~/.soda/env_vars.yml by running command | cat ~/.soda/env_vars.yml | Then run the soda analyze command ``` 4. Optionally, use the following commands to review the contents of the two YAML files you created. You do not need to adjust any contents as Soda SQL has already configured the warehouse connection details. * `cat ./warehouse.yml` * `cat ~/.soda/env_vars.yml` ## Prepare default tests 1. Use the `soda analyze` command to get Soda SQL to sift through the contents of your warehouse and automatically prepare a scan YAML file for each table. Soda SQL puts the YAML files in a new `/tables` directory in the warehouse directory you created. Read more about [scan YAML]({% link documentation/scan.md %}create-a-scan-yaml-file) files. Command: ```shell soda analyze ``` Output: ``` | Analyzing warehouse.yml ... | Querying warehouse for tables | Creating tables directory tables | Executing SQL query: SELECT table_name FROM information_schema.tables WHERE lower(table_schema)='public' | SQL took 0:00:00.007998 | Creating tables/demodata.yml ... | Executing SQL query: ... | SQL took 0:00:00.000647 | Next run 'soda scan warehouse.yml tables/demodata.yml' to calculate measurements and run tests ``` 2. Use the following command to review the contents of the new scan YAML file that Soda SQL created and named `demodata.yml`. Command: ```shell cat ./tables/demodata.yml ``` Output: ![tutorial-output](../assets/images/tutorial-output.png){:height=\"340px\" width=\"340px\"} 3. Note the three tests that Soda SQL configured in `demodata.yml`. When it created this file, Soda SQL pre-populated it with the `test` and `metric` configurations it deemed useful based on the data in the table it analyzed. Read more about the [Anatomy of the scan YAML file]({% link documentation/scan.md %}#anatomy-of-the-scan-yaml-file). ## Run a scan 1. Use the `soda scan` command to run tests against the data in the demodata warehouse. As input, the command requires the name of the warehouse to scan, and the path and name of the table in the warehouse. Command: ```shell soda scan warehouse.yml tables/demodata.yml ``` 2. Examine the output of the command, in particular the lines at the bottom that indicate the results of the tests Soda SQL ran against your data. In this example, all the tests passed which indicates that there are no issues with the data. Output: ```shell | Soda CLI version 2.0.0 beta | Scanning demodata in ./soda_sql_tutorial ... | Environment variable POSTGRES_PASSWORD is not set | Executing SQL query: SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE lower(table_name) = 'demodata' AND table_catalog = 'sodasql' AND table_schema = 'public' | SQL took 0:00:00.029199 | 6 columns: | id character varying | name character varying | size integer | date date | feepct character varying | country character varying | Query measurement: schema = id character varying, name character varying, size integer, date date, feepct character varying, country character varying | Executing SQL query: SELECT COUNT(*), COUNT(id), MIN(LENGTH(id)), MAX(LENGTH(id)), COUNT(name), MIN(LENGTH(name)), MAX(LENGTH(name)), COUNT(size), ... | missing_count(country) = 0 | values_percentage(country) = 100.0 | All good. 38 measurements computed. No tests failed. ``` 3. If you used Docker to create a sample PostgreSQL warehouse for this tutorial, be sure to execute the following commands to stop the container. ```shell $ docker stop soda_sql_tutorial_db $ docker volume rm soda_sql_tutorial_postgres ``` ## Go further * [Post your feedback](https://github.com/sodadata/soda-sql/discussions) about this tutorial! * Learn more about [How Soda SQL works]({% link documentation/concepts.md %}). * Learn more about the [scan YAML file]({% link documentation/scan.md %}) and how to [run scans]({% link documentation/scan.md %}#run-a-scan). * Learn more about configuring [tests]({% link documentation/tests.md %}) and [metrics]({% link documentation/sql_metrics.md %}). * Configure your warehouse YAML to [connect to your warehouse]({% link documentation/warehouse_types.md %}). ",
    "url": "/soda-sql/getting-started/5_min_tutorial.html",
    "relUrl": "/getting-started/5_min_tutorial.html"
  },"1": {
    "doc": "Get Started",
    "title": "Get Started",
    "content": ". # Soda SQL **Soda SQL** is an open-source command-line tool. It utilizes user-defined input to prepare SQL queries that run tests on tables in a database to find invalid, missing, or unexpected data. When tests fail, they surface \"bad\" data that you and your data engineering team can fix, ensuring downstream analysts are using \"good\" data to make decisions. Use Soda SQL to scan a variety of data warehouses: | Apache Hive AWS Athena AWS Redshift GCP BigQuery | Microsoft SQL Server PostgreSQL Snowflake | . * [Install Soda SQL]({% link getting-started/installation.md %}) from your command-line interface. * Follow the [Quick start tutorial]({% link getting-started/5_min_tutorial.md %}) to set up Soda SQL and run your first scan in minutes. * Learn the [Basics of Soda SQL]({% link documentation/concepts.md %}#soda-sql-basics). * Use Soda SQL with your [data orchestration tool]({% link documentation/orchestrate_scans.md %}) to automate data monitoring and testing. * Use Soda SQL as a stand-alone solution, or [connect to a free Soda Cloud account]({% link documentation/connect_to_cloud.md %}) to use the web app to monitor data quality. * Help us make Soda SQL even better! Join our [developer community]({% link community.md %}) and [contribute](https://github.com/sodadata/soda-sql/blob/main/CONTRIBUTING.md). # Soda Cloud Connect Soda SQL to a free **Soda Cloud** account where you and your team can use the web application to monitor test results and collaborate to keep your data issue-free. * Set up your free Soda Cloud account at [cloud.soda.io](https://cloud.soda.io/signup). * Soda SQL can run without Soda Cloud, but Soda Cloud Free and Teams editions depend upon Soda SQL! Install Soda SQL, then [connect it to your Soda Cloud Account]({% link documentation/connect_to_cloud.md %}). * Learn more about [Soda Cloud architecture]({% link documentation/soda-cloud-architecture.md %}). * [Create monitors and alerts]({% link documentation/monitors.md %}) to notify your team about data issues. ",
    "url": "/soda-sql/",
    "relUrl": "/"
  },"2": {
    "doc": "Soda SQL CLI commands",
    "title": "Soda SQL CLI commands",
    "content": "# Soda SQL CLI commands | Command | Description | ------- | ----------- | `soda analyze` | Analyzes the contents of your warehouse and automatically prepares a scan YAML file for each table. Soda SQL puts the YAML files in the `/tables` directory inside the warehouse directory. See [Create a scan YAML file]({% link documentation/scan.md %}#create-a-scan-yaml-file) for details.| `soda create` | Creates a new `warehouse.yml` file and prepares credentials in your `~/.soda/env_vars.yml`. Soda SQL does not overwrite or remove and existing environment variables, it only adds new. See [Create a warehouse YAML file]({% link documentation/warehouse.md %}#create-a-warehouse-yaml-file) for details. | `soda scan` | Uses the configurations in your scan YAML file to prepare, then run SQL queries against the data in your warehouse. See [Run a scan]({% link documentation/scan.md %}#run-a-scan) for details. | ## List of commands To see a list of Soda SQL command-line interface (CLI) commands, use the `soda` command. Command: ```shell $ soda ``` Output: ```shell Usage: soda [OPTIONS] COMMAND [ARGS]... Soda CLI version 2.x.xxx Options: --help Show this message and exit. Commands: analyze Analyzes tables in the warehouse and creates scan YAML files... create Creates a new warehouse.yml file and prepares credentials in your... init Renamed to `soda analyze` scan Computes all measurements and runs all tests on one table. ``` ## List of parameters To see a list of configurable parameters for each command, use the command-line help. ```shell $ soda create --help $ soda analyze --help $ soda scan --help ``` ## Go further * Learn [How Soda SQL works]({% link documentation/concepts.md %}). * [Install Soda SQL]({% link getting-started/installation.md %}). * [Set up Soda SQL]({% link getting-started/5_min_tutorial.md %}) and run your first scan. ",
    "url": "/soda-sql/documentation/cli.html",
    "relUrl": "/documentation/cli.html"
  },"3": {
    "doc": "Community",
    "title": "Community",
    "content": "# Community Soda SQL is open-source software distributed under the Apache License 2.0. ## Contribute We welcome and encourage any kind of contributions and suggestions for improvement! Soda SQL project: [github.com/sodadata/soda-sql](https://github.com/sodadata/soda-sql/) Join the conversation in [Soda's Slack community](https://join.slack.com/t/soda-community/shared_invite/zt-m77gajo1-nXJF7JtbbRht2zwaiLb9pg). Contact us using [GitHub discussions](https://github.com/sodadata/soda-sql/discussions) to: * ask a question * post a problem * share your feedback * suggest an improvement or contribution Write [documentation](#documentation-guidelines) for Soda SQL. Read more about how to [contribute to Soda SQL](https://github.com/sodadata/soda-sql/blob/main/CONTRIBUTING.md). ## Roadmap We have a long and exciting path ahead for developing Soda SQL and beyond. Check out our [project roadmap](https://github.com/sodadata/soda-sql/projects/1) on GitHub. Do you have a feature request? Want to help us develop CLI tools for other SQL engines? [Create an issue](https://github.com/sodadata/soda-sql/issues/new) on GitHub. ## Documentation guidelines Join us in our mission to help users become productive and confident using Soda software. The following outlines the workflow to contribute. 1. [Set up docs tooling](#set-up-docs-tooling) locally on your machine and clone the GitHub repo. 2. Create a new branch for your work. Include the word `docs` in the name of your branch. 3. Follow the [style guidelines](#style-guidelines) to edit existing or write new content using [markdown](#use-jekyll-markdown). 4. Adjust the nav order of any new files in the header of the file. 5. Spell check your content (select all and copy to Google Docs for a thorough check) and test all links. 6. Commit your contributions, create a pull request, and request a review, if you wish. 7. When all tests pass, merge your pull request. 8. Celebrate your new life as a published author! ### Set up docs tooling Soda uses the following tools to build and publish documentation. - [GitHub](https://github.com/sodadata/soda-sql) to store content - [Jekyll](https://jekyllrb.com/docs/) to build and serve content - [Just the Docs](https://pmarsceill.github.io/just-the-docs/) to apply a visual theme To contribute to Soda documentation, set up your local system to author and preview content before committing it. 1. Jekyll requires Ruby 2.4.0 or higher. If necessary, upgrade or install Ruby locally from the command-line. ```shell brew install ruby ``` 2. Add Ruby path to your `~/.bash_profile`. ```shell $ echo 'export PATH=\"/usr/local/opt/ruby/bin:$PATH\"' >> ~/.bash_profile ``` 3. Relaunch your command-line interface, then check your Ruby setup. ```shell $ which ruby /usr/local/opt/ruby/bin/ruby $ ruby -v ruby 3.0.0p0 (2020-12-25 revision 95aff21468) [x86_64-darwin20] ``` 4. Install two Ruby gems: bundler and jekyll. ```shell $ gem install --user-install bundler jekyll ``` 5. Add gem path to your `~/.bash_profile`. ```shell $ echo 'export PATH=\"/Users/Janet/.gem/ruby/3.0.0/bin:$PATH\"' >> ~/.bash_profile ``` 6. Check that `GEM PATHS` point to your home directory. ```shell $ gem env ... GEM PATHS: - /usr/local/lib/ruby/gems/3.0.0 - /Users/me/.gem/ruby/3.0.0 - /usr/local/Cellar/ruby/3.0.0_1/lib/ruby/gems/3.0.0 ``` 7. Clone the soda-sql repo from GitHub. 8. Open the cloned repo locally in your favorite code editor such as Visual Code or Sublime. 9. From the command-line, navigate to the directory that contains the cloned repo, then run the following to build and serve docs locally. ```shell $ bundle exec jekyll serve ``` 10. In a browser, navigate to [http://localhost:4000/soda-sql/](http://localhost:4000/soda-sql/) to see a preview of the docs site locally. Make changes to docs files in your code editor, save the files, then refresh your browser to preview your changes. ### Troubleshoot **Problem:** When I run `bundle exec jekyll serve`, I get this error: ``` $ bundle exec jekyll serve ... /usr/local/lib/ruby/gems/3.0.0/gems/jekyll-4.2.0/lib/jekyll/commands/serve/servlet.rb:3:in `require': cannot load such file -- webrick (LoadError) ``` **Solution:** Ruby 3.0.0 no longer includes the `webrick 1.7.0` gem that jekyll needs to build and serve docs. Install the gem locally. 1. In your code editor, open `docs` > `Gemfile`. 2. Add the following to the file and save. ```ruby $ gem \"webrick\" ``` 3. From the command-line, run `bundle install`. 4. Run `bundle exec jekyll serve`. 5. Before you commit your changes to the repo, be sure to discard your changes to the Gemfile. Just add the `webrick` gem again the next time you open a new branch. ### Style guidelines Soda uses the [Splunk Style Guide](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Howtouse) for writing documentation. For any questions about style or language that are not listed below, refer to the Splunk Style Guide for guidance. Language: - Use American English. - Use [plain](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Technicallanguage) language. Do not use jargon, colloquialisms, or meme references. - Use [unbiased](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Inclusivity) language. For example, instead of \"whitelist\" and \"blacklist\", use \"passlist\" and \"denylist\". - Avoid writing non-essential content such as welcome messages or backstory. - When referring to a collection of files, use \"directory\", not \"folder\". - Use \"you\" and \"your\" to refer to the reader. - Do not refer to Soda, the company, as participants in documentation: \"we\", \"us\", \"let's\", \"our\". - Use [active voice](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Activeandpresent). - Use present tense and imperative mood. See the [Set up docs tooling](#set-up-docs-tooling) section above for an example. - Avoid the subjunctive mood: \"should\", \"would\", \"could\". - Make the language of your lists [parallel](https://ewriteonline.com/how-and-why-to-make-your-lists-parallel-and-what-does-parallel-mean/). Good practice: - Never write an FAQ page or section. FAQs are a randomly organized bucket of content that put the burden on the reader to find what they need. Instead, consciously think about when and where a user needs the information and include it there. - Include code snippets and commands. - Limit inclusion of screencaps. These images are hard to keep up-to-date as the product evolves. - Include diagrams. - Do not use \"Note:\" sections. Exception: to indicate incompatibility or a known issue. - Use includes rather than repeat or re-explain something. Formatting: - Use **bold** for the first time you mention a product name or feature in a document. See [Warehouse YAML]({% link documentation/warehouse.md %}) for an example. Otherwise, use it sparingly. Too much bold font renders the format meaningless. - Use *italics* sparingly for emphasis on the negative. For example, \"Do *not* share your login credentials.\" - Use sentence case for all titles and headings. - Use H1 headings for the page title. Use H2 and H3 as subheadings. Use H4 headings to introduce example code snippets. - Never stack headings with no content between them. Add content or remove a heading, likely the latter so as to avoid adding non-essential text. - Use [bulleted lists](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Bulletlists) for non-linear lists. - Use [numbered lists](https://docs.splunk.com/Documentation/StyleGuide/current/StyleGuide/Tasklists) for procedures or ordered tasks. - Use relative links to link to other files or sections in Soda documentation. - Use hard-coded links to link to external sources. - Liberally include links to the [Glossary]({% link documentation/glossary.md %}), but only link the first instance of a term on a page, not all instances. Content: - Categorize your new content according to the following macro groups: - Concepts - content that explains in general, without including procedural steps. Characterized by a title that does not use present tense imperative such as, \"How Soda SQL works\" or \"Metrics\". - Tasks - content that describes the steps a user takes to complete a task or reach a goal. Characterized by a title that is in present tense imperative such as, \"Install Soda SQL\" or \"Apply filters\". - Reference - content that presents lists or tables of reference material such as error codes or glossary. - Produce content that focuses on how to achieve a goal or solve a problem and, insofar as it is practical, is inclusive of all products. Avoid creating documentation that focuses on how to use a single product. For example, instead of writing two documents -- one for \"Troubleshoot Soda SQL\" and one for \"Troubleshoot Soda Cloud\" -- write one Troubleshoot document that offers guidance for both tools. - Remember that Every Page is Page One for your reader. Most people enter your docs by clicking on the result of a Google search, so they could land anywhere and you should assume your new page is the first page that a new reader lands on. Give them the context for what they are reading, lots of \"escape hatches\" to the glossary or pre-requisite procedures, and instructions on what to read next in a \"Go further\" section at the bottom of all Concept or Task pages. ### Use Jekyll markdown Kramdown is the default markdown renderer that Jekyll uses. {% raw %} Insert image: Add a `png` file of your logically-named image to the `docs/assets/images` directory, then add this code: ``` ![scan-anatomy](../assets/images/scan-anatomy.png){:height=\"440px\" width=\"440px\"} ``` Includes: Add a markdown file of your logically-named include content to the `docs/_includes` directory, then add this code: ``` {% include run-a-scan.md %} ``` Relative links: ``` [warehouse yaml]({% link documentation/warehouse.md %}) [airflow_bash.py](../../examples/airflow_bash.py) ``` Link to anchor: ``` [warehouse yaml]({% link documentation/warehouse.md %}#to-anchor) ``` Link to section on same page: ``` [example](#example-tests-using-a-column-metric) ``` External link: ``` [Wikipedia](https://en.wikipedia.org) ``` Comment out: ``` ``` {% endraw %} ",
    "url": "/soda-sql/community.html",
    "relUrl": "/community.html"
  },"4": {
    "doc": "How Soda SQL works",
    "title": "How Soda SQL works",
    "content": "# How Soda SQL works **Soda SQL** is an open-source command-line tool. It utilizes user-defined input to prepare SQL queries that run tests on tables in a database to find invalid, missing, or unexpected data. When tests fail, they surface the data that you defined as \"bad\" in the tests. Armed with this information, you and your data engineering team can diagnose where the \"bad\" data entered your data pipeline and take steps to prioritize and resolve issues based on downstream impact. Use Soda SQL on its own to manually or programmatically scan the data that your organization uses to make decisions. Optionally, you can integrate Soda SQL with your data orchestration tool to schedule scans and automate actions based on scan results. Further, you can connect Soda SQL to a free **Soda Cloud** account where you and your team can use the web application to monitor test results and collaborate to keep your data issue-free. ## Soda SQL basics This open-source, command-line tool exists to enable Data Engineers to access and test data inside [warehouses]({% link documentation/glossary.md %}#warehouse). The first of what will soon be many such developer tools, Soda SQL allows you to perform three basic tasks: - connect to your warehouse, - define tests for \"bad\" data, and, - scan your [table]({% link documentation/glossary.md %}#table) to run tests for \"bad\" data. To **connect** to a data warehouse such as Snowflake, AWS Athena, or Google Cloud Products BigQuery, you use two files that Soda SQL creates for you when you run the `soda create` CLI command: - a `warehouse.yml` file which stores access details for your warehouse, and, - an `env_vars.yml` file which securely stores warehouse login credentials. #### Warehouse YAML example ```yaml name: soda_sql_tutorial connection: type: postgres host: localhost username: env_var(POSTGRES_USERNAME) password: env_var(POSTGRES_PASSWORD) database: sodasql schema: public ``` #### env_vars YAML example ```yaml soda_sql_tutorial: POSTGRES_USERNAME: xxxxxx POSTGRES_PASSWORD: yyyyyy ``` To **define** the data quality tests that Soda SQL runs against a table in your warehouse, you use the scan YAML files that Soda SQL creates when you run the `soda analyze` CLI command. Soda SQL uses the Warehouse YAML file to connect to your warehouse and analyze the tables in it. For every table that exists, Soda SQL creates a corresponding scan YAML file and automatically populates it with tests it deems relevant for your data. You can keep these default tests intact, or you can adjust them or add more tests to fine-tune your search for \"bad\" data. For example, you can define tests that look for things like, non-UUID entries in the id column of a table, or zero values in a commission percentage column. See [Scan YAML]({% link documentation/scan.md %}) for much more detail on the contents of this file. #### Scan YAML example ```yaml table_name: demodata metrics: - row_count - missing_count - missing_percentage - ... tests: - row_count > 0 columns: id: valid_format: uuid tests: - invalid_percentage == 0 ``` To **scan** your data, you use the `soda scan` CLI command. Soda SQL uses the input in the scan YAML file to prepare SQL queries that it runs against the data in a table in a warehouse. All tests return true or false; if true, the test passed and you know your data is sound; if false, the test fails which means the scan discovered data that falls outside the expected or acceptable parameters you defined in your test. ## Soda SQL operation Imagine you have installed Soda SQL, you have run the `soda create` command to set up your Warehouse and env_vars YAML files, and you have added your warehouse login credentials to the env_vars YAML. You have run `soda analyze`, and you have some new scan YAML files in your `/tables` directory that map to tables in your database. You are ready to scan! The following image illustrates what Soda SQL does when you initiate a scan. ![soda-operation](../assets/images/soda-operation.png){:height=\"800px\" width=\"800px\"} **1** - You trigger a scan using the `soda scan` CLI command from your [warehouse directory]({% link documentation/glossary.md %}#warehouse-directory). The scan specifies which Warehouse YAML and scan YAML files to use, which amounts to telling it which table in which warehouse to scan. **2** - Soda SQL uses the tests in the scan YAML to prepare SQL queries that it runs on the tables in your warehouse. **3** - When Soda SQL runs a scan, it performs the following actions: - fetches column metadata (column name, type, and nullable) - executes a single aggregation query that computes aggregate metrics for multiple columns, such as `missing`, `min`, or `max` - for each column, executes several more queries, including `distinct_count`, `unique_count`, and `valid_count` **4** - {% include scan-output.md %} ## Soda SQL automation and integrations To automate scans on your data, you can use the **Soda SQL Python library** to programmatically execute scans. Based on a set of conditions or a specific schedule of events, you can instruct Soda SQL to automatically scan for \"bad\" data. For example, you may wish to scan your data at several points along your data pipeline, perhaps when new data enters a warehouse, after it is transformed, and before it is exported to another warehouse. Refer to [Programmatic scan]({% link documentation/programmatic_scan.md %}) for details. Alternatively, you can integrate Soda SQL with a **data orchestration tool** such as, Airflow, Dagster, or dbt, to schedule automated scans. You can also configure actions that the orchestration tool can take based on scan output. For example, if the output of a scan reveals a large number of failed tests, the orchestration tool can automatically quarantine the \"bad\" data or block it from contaminating your data pipeline. Refer to [Orchestrate scans]({% link documentation/orchestrate_scans.md %}) for details. Additionally, you can integrate Soda SQL with a **Soda Cloud** account. This free, cloud-based web application integrates with your Soda SQL implementation giving your team broader visibility into your organization's data quality. Soda SQL pushes scan results to your Soda Cloud account where you can use the web app to examine the results. Notably, Soda SQL only ever pushes *metadata* to the cloud; all your data stays inside your private network. Learn more about [Soda Cloud architecture]({% link documentation/soda-cloud-architecture.md %}). Though you do not have to set up and ingrate a Soda Cloud account in order to use Soda SQL, the web app serves to complement the CLI tool, giving you a non-CLI method of examining data quality. Use Soda Cloud to: - collaborate with team members to review details of scan results that can help you to diagnose data issues - use monitors to view stored [scan output]({% link documentation/scan.md %}#scan-output) as a line graph that represents the volume of failed tests in each scan - empower others to [set quality thresholds]({% link documentation/monitors.md %}) that define \"good\" data - set up and [send alert notifications]({% link documentation/monitors.md %}) when \"bad\" data enters your data pipeline To connect Soda SQL to Soda Cloud, you create API keys in your Soda Cloud account and configure them as connection credentials in your Warehouse and env_vars YAML files. See [Connect to Soda Cloud]({% link documentation/connect_to_cloud.md %}) for details. ## Go further * Learn more about the contents of the [Scan YAML]({% link documentation/scan.md %}) file. * Learn more about the [Metrics]({% link documentation/sql_metrics.md %}) you can use to define [Tests]({% link documentation/tests.md %}). * Learn how to [Connect to Soda Cloud]({% link documentation/connect_to_cloud.md %}). * See how to prepare [programmatic scans]({% link documentation/programmatic_scan.md %}) of your data. ",
    "url": "/soda-sql/documentation/concepts.html",
    "relUrl": "/documentation/concepts.html"
  },"5": {
    "doc": "Configure Soda SQL",
    "title": "Configure Soda SQL",
    "content": "# Configure Soda SQL After you [install Soda SQL]({% link getting-started/installation.md %}), you must create files and configure a few settings before you can run a scan. ## Overview 1. Create a [warehouse directory]({% link documentation/glossary.md %}#warehouse-directory) in which to store your warehouse YAML file and `/tables` directory. 2. Create a [warehouse YAML file]({% link documentation/glossary.md %}#warehouse-yaml) and an [env_vars YAML file]({% link documentation/glossary.md %}#env_vars-yaml), then adjust the contents of each to input your [warehouse]({% link documentation/glossary.md %}#warehouse) connection details. 3. Create a [scan YAML file]({% link documentation/glossary.md %}#scan-yaml) for each [table]({% link documentation/glossary.md %}#table) that exists in your warehouse. The scan YAML files store the test criteria that Soda SQL uses to prepare SQL queries that [scan]({% link documentation/glossary.md %}#scan) your warehouse. 4. Adjust the contents of your new scan YAML files to add the [tests]({% link documentation/glossary.md %}#test) you want to run on your data to check for quality. Consider following the [Quick start tutorial]({% link getting-started/5_min_tutorial.md %}) that guides you through configuration and scanning. ## Configure 1. Use your command-line interface to create, then navigate to a new Soda SQL warehouse directory in your environment. The warehouse directory stores your warehouse YAML files and `/tables` directory. The example below creates a directory named `soda_warehouse_directory`. ```shell $ mkdir soda_warehouse_directory $ cd soda_warehouse_directory ``` 2. Use the `soda create` command to create and pre-populate two files that enable you to configure connection details for Soda SQL to access your warehouse: * a `warehouse.yml` file which stores access details for your warehouse ([read more]({% link documentation/warehouse.md %})) * an `env_vars.yml` file which securely stores warehouse login credentials ([read more]({% link documentation/warehouse.md %}#env_vars-yaml)) ```shell $ soda create -d yourdbname -u dbusername -w soda_warehouse_directory typeofdb ``` 3. Use a code editor to open the `warehouse.yml` file that Soda SQL created and put in your warehouse directory. Refer to [Set warehouse configurations]({% link documentation/warehouse_types.md %}) to adjust the configuration details according to the type of warehouse you use, then save the file. Example warehouse YAML ```shell name: soda_warehouse_directory connection: type: postgres host: localhost username: env_var(POSTGRES_USERNAME) password: env_var(POSTGRES_PASSWORD) database: sodasql schema: public ``` 4. Use a code editor to open the `env_vars.yml` that Soda SQL created and put in your local user home directory as a hidden file (`~/.soda/env_vars.yml`). Input your warehouse login credentials then save the file. Example env_vars YAML ```shell soda_warehouse_directory: POSTGRES_USERNAME: someusername POSTGRES_PASSWORD: somepassword ``` 5. In your command-line interface, use the `soda analyze` command to get Soda SQL to sift through the contents of your warehouse and automatically prepare a scan YAML file for each table. Soda SQL uses the name of the table to name each YAML file which it puts a new `/tables` directory in the warehouse directory. ```shell soda analyze ``` 6. Use a code editor to open one of your new scan YAML files. Soda SQL pre-populated the YAML file with default metrics and tests that it deemed useful for the kind of data in the table. See [scan YAML]({% link documentation/scan.md %}#anatomy-of-the-scan-yaml-file). Adjust the contents of the YAML file to define the tests that you want Soda SQL to conduct when it runs a scan on this table in your warehouse. Refer to [Metrics]({% link documentation/sql_metrics.md %}) and [Tests]({% link documentation/tests.md %}) for details. Example scan YAML ![configure yaml](../assets/images/configure-yaml.png){:height=\"275px\" width=\"275px\"} 7. With your configuration complete, [run your first scan]({% link documentation/scan.md %}#run-a-scan). ## Go further * Learn more about [How Soda SQL works]({% link documentation/concepts.md %}). * Learn more about the [scan YAML file]({% link documentation/scan.md %}). * Learn more about configuring [tests]({% link documentation/tests.md %}) and [metrics]({% link documentation/sql_metrics.md %}). ",
    "url": "/soda-sql/getting-started/configure.html",
    "relUrl": "/getting-started/configure.html"
  },"6": {
    "doc": "Connect to Soda Cloud",
    "title": "Connect to Soda Cloud",
    "content": "# Connect to Soda Cloud To use the **[Soda Cloud]({% link documentation/glossary.md %}#soda-cloud)** web application to monitor your data, you must install and configure the **[Soda SQL]({% link documentation/glossary.md %}#soda-sql)** command-line tool, then connect it to your Soda Cloud account. Soda SQL uses an API to connect to Soda Cloud. To use the API, you must generate API keys in your Soda Cloud account, then add them to the [warehouse YAML]({% link documentation/warehouse.md %}) file that Soda SQL created. 1. If you have not already done so, create a Soda Cloud account at [cloud.soda.io](https://cloud.soda.io/signup). 2. Use the instructions in [Install Soda SQL]({% link getting-started/installation.md %}) to install Soda SQL. 3. Follow steps in the [Quick start tutorial]({% link getting-started/5_min_tutorial.md %}) to create your warehouse YAML file, connect to your [warehouse]({% link documentation/glossary.md %}#warehouse), analyze your [tables]({% link documentation/glossary.md %}#table), and run a [scan]({% link documentation/glossary.md %}#scan) on the data. 4. Open the `warehouse.yml` file in a text editor, then add the following to the file: ```shell soda_account: host: cloud.soda.io api_key_id: env_var(API_PUBLIC) api_key_secret: env_var(API_PRIVATE) ``` 5. Save the `warehouse.yml` file. 6. Open your `~/.soda/env_vars.yml` file in a text editor, then add the following to the file: ```shell [warehouse_name]: ... API_PUBLIC: API_PRIVATE: ``` 7. In Soda Cloud, navigate to your **Profile** page to generate new API keys. * Copy the Public key, then paste it into the `env_vars.yml` file as the value for `API_PUBLIC`. * Copy the Private key, then paste it into the `env_vars.yml` file as the value for `API_PRIVATE`. 8. Save the changes to the `env_vars.yml` file. Close the API Keys create dialog box in your Soda Cloud account. 9. From the command-line, use Soda SQL to scan the tables in your warehouse again. ```shell $ soda scan warehouse.yml tables/[dbtablename].yml ``` 10. Navigate to your Soda Cloud account and refresh the browser. Review the results of your scan in Monitor Results. ## Go further * Learn how to [create monitors and alerts]({% link documentation/monitors.md %}). * Learn more about [Soda Cloud architecture]({% link documentation/soda-cloud-architecture.md %}). * Learn more about the [warehouse yaml]({% link documentation/warehouse.md %}) file. * Learn more about the [anatomy of a scan]({% link documentation/scan.md %}#anatomy-of-the-scan-yaml-file) ",
    "url": "/soda-sql/documentation/connect_to_cloud.html",
    "relUrl": "/documentation/connect_to_cloud.html"
  },"7": {
    "doc": "Apply filters",
    "title": "Apply filters",
    "content": "# Apply filters To test specific portions of data for quality, you can apply dynamic **filters** when you [scan]({% link documentation/glossary.md %}#scan) data in your [warehouse]({% link documentation/glossary.md %}#warehouse). To do so, you define a filter [configuration key]({% link documentation/glossary.md %}#configuration-key) in your [scan YAML]({% link documentation/glossary.md %}#scan-yaml) file, then add a variable to the `soda scan` command that specifies a portion of data for Soda SQL to scan instead of scanning a larger data set. When you add a variable, Soda SQL adds a filter to the `WHERE` clause of the SQL queries it creates to scan your data. Refer to [How Soda SQL works]({% link documentation/concepts.md %}) to learn more. For example, where a `CUSTOMER_TRANSACTIONS` [table]({% link documentation/glossary.md %}#table) has a `DATE` column, you may wish to run a scan only against the newest data added to the table. In such a case, you can apply a filter for a specific date so that Soda SQL only scans data associated with that date. ```shell $ soda scan -v date=2021-01-12 warehouse.yml tables/customer_transactions.yml ``` Similarly, if the table has a column for `COUNTRY`, you can apply a filter that scans only the data that pertains to the country you specify as a variable in the `soda scan` command. ```shell $ soda scan -v country=FRA warehouse.yml tables/customer_transactions.yml ``` ## Configure a filter 1. Open the [scan YAML]({% link documentation/scan.md %}) file associated with the table on which you want to run filtered scans. The scan YAML files are usually located in the [warehouse directory]({% link documentation/glossary.md %}#warehouse-directory) of your Soda SQL project. 2. To the file, add a filter configuration key as per the following example. Be sure to use quotes for input that is in text format. ```yaml table_name: CUSTOMER_TRANSACTIONS filter: \"date = DATE '{{ date }}'\" metrics: ... columns: ... ``` 3. When you define a variable in your scan YAML file, Soda SQL applies the filter to all tests *except* tests defined in SQL metrics. To apply a filter to SQL metrics tests, be sure to explicitly define the variable in your SQL query. Refer to [Variables in SQL metrics]({% link documentation/sql_metrics.md %}#variables-in-sql-metrics) 4. Save the changes to the YAML file, then run a filtered scan by adding a variable to your `soda scan` command in your command-line interface. ```shell $ soda scan -v date=2021-01-12 warehouse.yml tables/customer_transactions.yml ``` For time-partitioned tables, consider configuring a separate scan YAML file for scans that use a filter. For example: * `customer_transactions.yml` with no filter defined in the file, so you can scan all the data in the table without applying a date filter ```shell $ soda scan warehouse.yml tables/customer_transactions.yml ``` * `customer_transactions_tp.yml` with a date filter defined, so you can add a date variable to your `soda scan` command to scan against a date you specify ```shell $ soda scan -v date=2021-01-12 warehouse.yml tables/customer_transactions_tp.yml ``` ## Configure a filter in a programmatic scan Alternatively, you can use variables when programmatically invoking a Soda SQL scan. Refer to [Configure programmatic scans]({% link documentation/programmatic_scan.md %}). 1. Follow steps 1 - 3 above to prepare your scan YAML file. 2. Configure your programmatic scan to include variables as per the following example. ```python scan_builder = ScanBuilder() scan_builder.warehouse_yml_file = 'warehouse.yml' scan_builder.scan_yml_file = 'tables/my_table.yml' scan_builder.variables = { 'date': '2021-06-12' } scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_test_failures(): print('Scan has test failures, stop the pipeline') ``` ## Go further * Learn more about configuring your [scan YAML]({% link documentation/scan.md %}) file. * Learn more about [how Soda SQL works]({% link documentation/concepts.md %}) file. ",
    "url": "/soda-sql/documentation/filtering.html",
    "relUrl": "/documentation/filtering.html"
  },"8": {
    "doc": "Glossary",
    "title": "Glossary",
    "content": "# Glossary ### alert A setting that you configure in Soda Cloud by specifying key:value thresholds which, if passed, trigger a notification. See also: [notification](#notification). ### analyze A Soda SQL CLI command that sifts through the contents of your database and automatically prepares a scan YAML file for each table. See [Create a scan YAML file]({% link documentation/scan.md %}#create-a-scan-yaml-file). ### column A column in a table in your warehouse. ### column configuration key The key in the key-value pair that you use to define what qualifies as a valid value in a column. A Soda SQL scan uses the value of a column configuration key to determine if it should pass or fail a test on a column. For example, in `valid_format: UUID` , `valid_format` is a column configuration key and `UUID` is the only format of the data in the column that Soda SQL considers valid. See [Column metrics]({% link documentation/sql_metrics.md %}#column-metrics). ### column metric A property of the data of a single column in your database. Use a column metric to define tests that apply to specific columns in a table during a scan. See [Column metrics]({% link documentation/sql_metrics.md %}#column-metrics). ### configuration key The key in the key-value pair that you use to define configuration in your scan YAML file. See [Scan YAML configuration keys]({% link documentation/scan.md %}#scan-yaml-configuration-keys). ### create A Soda SQL CLI command that creates a warehouse directory. ### default metric An out-of-the-box metric that you can configure in a scan YAML file. By contrast, you can use [SQL metrics]({% link documentation/sql_metrics.md %}#sql-metrics) to define custom metrics in a scan YAML file. ### env_vars YAML The file in your local user home directory that stores your database login credentials. ### measurement The value for a metric that Soda SQL obtains during a scan. For example, in `row_count = 5`, `row_count` is the metric and `5` is the measurement. ### metric A property of the data in your database. See [Metrics]({% link documentation/sql_metrics.md %}). ### monitor A scan you define in Soda Cloud that tests the data in your database. ### notification A setting you configure in Soda Cloud that defines whom to notify when a data issue triggers an alert. See also: [alert](#alert). ### scan A Soda SQL CLI command that uses SQL queries to extract information about data in a database table. ### scan YAML The file in which you configure scan metrics and tests. Soda SQL uses the input from this file to prepare, then run SQL queries against your data. See [Scan YAML]({% link documentation/scan.md %}). ### Soda Cloud A free, web application that enables you to examine the results of Soda SQL scans and create monitors and alerts. To use Soda Cloud, you must set up and connect Soda SQL to your Soda cloud account. ### Soda SQL An open-source command-line tool that scans the data in your warehouse. You can use this as a stand-alone tool to monitor data quality from the command-line, or connect it to a Soda Cloud account to monitor your data using a web application. ### SQL metric A custom metric you define in your scan YAML file. SQL metrics essentially enable you to add SQL queries to your scan YAML file so that Soda SQL runs them during a scan. See [SQL metrics]({% link documentation/sql_metrics.md %}#sql-metrics). ### table A table in your warehouse. ### table metric A property of the data in a table in your database. Use a table metric to define tests that apply to all data in the table during a scan. See [Table metrics]({% link documentation/sql_metrics.md %}#table-metrics). ### test A Python expression that, during a scan, checks metrics to see if they match the parameters defined for a measurement. As a result of a scan, a test either passes or fails. See [Tests]({% link documentation/tests.md %}). ### warehouse A SQL engine or database that contains data that you wish to test and monitor. ### warehouse directory The top directory in the Soda SQL directory structure which contains your warehouse YAML file and, generally, your `/tables` directory. ### warehouse YAML The file in which you configure warehouse connection details and Soda Cloud connection details. See [Warehouse YAML]({% link documentation/warehouse.md %}) and [Connect to Soda Cloud]({% link documentation/connect_to_cloud.md %}). ",
    "url": "/soda-sql/documentation/glossary.html",
    "relUrl": "/documentation/glossary.html"
  },"9": {
    "doc": "Documentation",
    "title": "Documentation",
    "content": " ",
    "url": "/soda-sql/documentation/",
    "relUrl": "/documentation/"
  },"10": {
    "doc": "Install Soda SQL",
    "title": "Install Soda SQL",
    "content": "# Install Soda SQL Soda SQL is a command-line interface (CLI) tool that enables you to scan the data in your database to surface invalid, missing, or unexpected data. **[Compatibility](#ccompatibility) [Requirements](#requirements) [Install](#install) [Upgrade](#upgrade) [Troubleshoot](#troubleshoot) [Go further](#go-further)** ## Compatibility Use Soda SQL with any of the following data warehouses: - Apache Hive - AWS Athena - AWS Redshift - Google Cloud Platform BigQuery - Microsoft SQL Server - PostgreSQL - Snowflake ## Requirements To use Soda SQL, you must have installed the following on your system: - **Python 3.7** or greater. To check your existing version, use the CLI command: `python --version` - **Pip 21.0** or greater. To check your existing version, use the CLI command: `pip --version` For Linux users only, install the following: - On Debian Buster: `apt-get install g++ unixodbc-dev python3-dev libssl-dev libffi-dev` - On CentOS 8: `yum install gcc-c++ unixODBC-devel python38-devel libffi-devel openssl-devel` For MSSQL Server users only, install the following: - [SQLServer Driver](https://docs.microsoft.com/en-us/sql/connect/odbc/microsoft-odbc-driver-for-sql-server?view=sql-server-ver15) ## Install From your command-line interface tool, execute the following command: ``` $ pip install soda-sql ``` Optionally, you can install Soda SQL in a virtual environment. Execute the following commands one by one: ``` python3 -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install soda-sql ``` ## Upgrade To upgrade your existing Soda SQL tool to the latest version, use the following command: ```shell pip install soda-sql -U ``` ## Troubleshoot {% include troubleshoot-install.md %} ## Go further * [Configure Soda SQL]({% link getting-started/5_min_tutorial.md %}). * [Run your first scan]({% link documentation/scan.md %}#run-a-scan). * Learn [How Soda SQL works]({% link documentation/concepts.md %}). ",
    "url": "/soda-sql/getting-started/installation.html",
    "relUrl": "/getting-started/installation.html"
  },"11": {
    "doc": "Integrate with Slack",
    "title": "Integrate with Slack",
    "content": "# Integrate with Slack Integrate your Slack workspace in your [Soda Cloud]({% link documentation/glossary.md %}#soda-cloud) account so that Soda Cloud can send Slack notifications to your team when a data issue triggers an [alert]({% link documentation/glossary.md %}#alert). 1. In Soda Cloud, navigate to **Profile** > **Organization Settings** > **Integrations**, then follow the guided steps to authorize Soda Cloud to connect to your Slack workspace. 2. Select the all Slack channels to which you might send notifications when Soda finds an issue with your data, then **Save**. ## Go further * [Create an alert]({% link documentation/monitors.md %}) to send Slack notifications when a scan surfaces a data issue. * [Connect]({% link documentation/connect_to_cloud.md %}) Soda SQL to your Soda Cloud account. ",
    "url": "/soda-sql/documentation/integrate-slack.html",
    "relUrl": "/documentation/integrate-slack.html"
  },"12": {
    "doc": "Create monitors and alerts",
    "title": "Create monitors and alerts",
    "content": "# Create monitors and alerts Log in to **[Soda Cloud]({% link documentation/glossary.md %}#soda-cloud)** to create **[monitors]({% link documentation/glossary.md %}#monitor)**, and customize **[alerts]({% link documentation/glossary.md %}#alert)** that send **[notifications]({% link documentation/glossary.md %}#notification)** to your team when a [scan]({% link documentation/glossary.md %}#scan) surfaces data issues. Further, you can use monitors to track data quality over time. Soda Cloud stores your scan results and prepares charts that represent the volume of failed tests in each scan. These visualizations of your scan results enable you to see where your data quality is improving or deteriorating over time. ## Prerequisites 1. Create a free Soda Cloud account at [cloud.soda.io/signup](https://cloud.soda.io/signup). 2. [Install Soda SQL]({% link getting-started/installation.md %}) in your local or development environment. Soda SQL executes the scans, then pushes the results of its scans to your Soda Cloud account. 3. Use Soda SQL to [analyze]({% link documentation/scan.md %}#create-a-scan-yaml-file) the tables in your warehouse and [run your first scan]({% link documentation/scan.md %}#run-a-scan). 4. [Connect]({% link documentation/connect_to_cloud.md %}) Soda SQL to your Soda Cloud account. 5. (Optional) [Integrate with Slack]({% link documentation/integrate-slack.md %}) to enable Soda Cloud to send Slack notifications to your team. ## Create a monitor and an alert 1. In Soda Cloud, navigate to the **Monitor Results** table, then click the stacked dots to **Create Monitor**. Select the type `Metric`, then follow the guided steps to complete the setup. 2. Specify the Slack or email notifications you want to send when bad data triggers a **Critical Alert** or **Warning**, add a brief description of what your monitor tests, then **Save** your monitor. Your new monitor appears as a row in the **Monitor Results** table. 3. Access your command-line interface, then use Soda SQL to scan your data again. ``` shell $ soda scan warehouse.yml tables/yourtablename.yml ``` 4. Check your Slack channel or email inbox; when a scan surfaces data that triggers your alert, Soda Cloud sends a notification. 5. Return to **Monitor Results** in Soda Cloud and refresh your browser. Click the monitor to access details that can help you diagnose and solve the data issue. ## Go further * Learn more about [How Soda SQL works]({% link documentation/concepts.md %}) * Learn more about [Soda Cloud architecture]({% link documentation/soda-cloud-architecture.md %}). ",
    "url": "/soda-sql/documentation/monitors.html",
    "relUrl": "/documentation/monitors.html"
  },"13": {
    "doc": "Configure orchestrated scans",
    "title": "Configure orchestrated scans",
    "content": "# Configure orchestrated scans Integrate Soda SQL with a **data orchestration tool** such as, Airflow, Dagster, or dbt, to automate and schedule your search for \"bad\" data. Not only can you schedule [scans]({% link documentation/glossary.md %}#scan) of [warehouse]({% link documentation/glossary.md %}#warehouse) [tables]({% link documentation/glossary.md %}#table), you can also configure actions that the orchestration tool can take based on scan output. For example, if the output of a scan reveals a large number of failed tests, the orchestration tool can automatically block \"bad\" data from contaminating your data pipeline. Use the results of a Soda SQL scan to instruct your orchestration tool to: - block \"bad\" data from entering your data pipeline (for testing), or, - allow data to enter the pipeline (for monitoring). ![orchestrate](../assets/images/orchestrate.png){:height=\"800px\" width=\"800px\"} Follow the instructions that correspond to your data orchestration tool: [Apache Airflow using PythonVirtualenvOperator](#apache-airflow-using-pythonvirtualenvoperator) [Apache Airflow using PythonOperator](#apache-airflow-using-pythonoperator) [Apache Airflow using BashOperator](#apache-airflow-using-bashoperator) [Prefect using a custom Task](#prefect-using-a-custom-task) More coming soon ## Apache Airflow using PythonVirtualenvOperator Though you can install Soda SQL directly in your Airflow environment, the instructions below use PythonVirtualenvOperator to run Soda SQL scans in a different environment. This keeps Soda SQL software separate to prevent any dependency conflicts. 1. Install `virtualenv` in your main Airflow environment. 2. Set the following variables in your Airflow environment. ```python warehouse_yml = Variable.get('soda_sql_warehouse_yml_path') scan_yml = Variable.get('soda_sql_scan_yml_path') ``` 3. Configure as per the following example. ```python from airflow import DAG from airflow.models.variable import Variable from airflow.operators.python import PythonVirtualenvOperator from airflow.operators.dummy import DummyOperator from airflow.utils.dates import days_ago from datetime import timedelta default_args = { 'owner': 'soda_sql', 'retries': 1, 'retry_delay': timedelta(minutes=5), } def run_soda_scan(warehouse_yml_file, scan_yml_file): from sodasql.scan.scan_builder import ScanBuilder scan_builder = ScanBuilder() # Optionally you can directly build the warehouse dict from Airflow secrets/variables # and set scan_builder.warehouse_dict with values. scan_builder.warehouse_yml_file = warehouse_yml_file scan_builder.scan_yml_file = scan_yml_file scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_test_failures(): failures = scan_result.get_test_failures_count() raise ValueError(f\"Soda Scan found {failures} errors in your data!\") dag = DAG( 'soda_sql_python_venv_op', default_args=default_args, description='A simple Soda SQL scan DAG', schedule_interval=timedelta(days=1), start_date=days_ago(1), ) ingest_data_op = DummyOperator( task_id='ingest_data' ) soda_sql_scan_op = PythonVirtualenvOperator( task_id='soda_sql_scan_demodata', python_callable=run_soda_scan, requirements=[\"soda-sql==2.0.0b10\"], system_site_packages=False, op_kwargs={'warehouse_yml_file': warehouse_yml, 'scan_yml_file': scan_yml}, dag=dag ) publish_data_op = DummyOperator( task_id='publish_data' ) ingest_data_op >> soda_sql_scan_op >> publish_data_op ``` ## Apache Airflow using PythonOperator If you do not want to use a PythonVirtualenvOperator, which installs Soda SQL on invocation, you can use PythonOperator. 1. Set the following variables in your Airflow environment. ```python warehouse_yml = Variable.get('soda_sql_warehouse_yml_path') scan_yml = Variable.get('soda_sql_scan_yml_path') ``` 2. Configure as per the following example. ```python from airflow import DAG from airflow.models.variable import Variable from airflow.operators.python import PythonOperator from airflow.operators.dummy import DummyOperator from airflow.utils.dates import days_ago from datetime import timedelta from sodasql.scan.scan_builder import ScanBuilder from airflow.exceptions import AirflowFailException # Make sure that this variables are set in your Airflow warehouse_yml = Variable.get('soda_sql_warehouse_yml_path') scan_yml = Variable.get('soda_sql_scan_yml_path') default_args = { 'owner': 'soda_sql', 'retries': 1, 'retry_delay': timedelta(minutes=5), } def run_soda_scan(warehouse_yml_file, scan_yml_file): scan_builder = ScanBuilder() scan_builder.warehouse_yml_file = warehouse_yml_file scan_builder.scan_yml_file = scan_yml_file scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_test_failures(): failures = scan_result.get_test_failures_count() raise AirflowFailException(f\"Soda Scan found {failures} errors in your data!\") dag = DAG( 'soda_sql_python_op', default_args=default_args, description='A simple Soda SQL scan DAG', schedule_interval=timedelta(days=1), start_date=days_ago(1), ) ingest_data_op = DummyOperator( task_id='ingest_data' ) soda_sql_scan_op = PythonOperator( task_id='soda_sql_scan_demodata', python_callable=run_soda_scan, op_kwargs={'warehouse_yml_file': warehouse_yml, 'scan_yml_file': scan_yml}, dag=dag ) publish_data_op = DummyOperator( task_id='publish_data' ) ingest_data_op >> soda_sql_scan_op >> publish_data_op ``` ## Apache Airflow using BashOperator Invoke a Soda SQL scan using Airflow BashOperator. 1. Install Soda SQL in your Airflow environment. 2. Set the following variables in your Airflow environment. ```python warehouse_yml = Variable.get('soda_sql_warehouse_yml_path') scan_yml = Variable.get('soda_sql_scan_yml_path') ``` 3. Configure as per the following example. ```python # In this DAG, the `soda_sql_scan_demodata` task fails when the tests you # defined for the `demodata` table fail. This prevents the `publish_data_op` from # running. You can further customize the bash command to use different Soda SQL command # options, such as passing variables to the `soda scan` command. from airflow import DAG from airflow.models.variable import Variable from airflow.operators.bash import BashOperator from airflow.operators.dummy import DummyOperator from airflow.utils.dates import days_ago from datetime import timedelta # Use the same variable name that you used when you set your Airflow variables soda_sql_project_path = Variable.get('soda_sql_project_path') default_args = { 'owner': 'soda_sql', 'retries': 1, 'retry_delay': timedelta(minutes=5), } dag = DAG( 'soda_sql_scan', default_args=default_args, description='A simple Soda SQL scan DAG', schedule_interval=timedelta(days=1), start_date=days_ago(1), ) # A dummy operator to simulate data ingestion ingest_data_op = DummyOperator( task_id='ingest_data' ) # Soda SQL Scan which runs the appropriate table scan for the ingestion soda_sql_scan_op = BashOperator( task_id='soda_sql_scan_demodata', bash_command=f'cd {soda_sql_project_path} && soda scan warehouse.yml tables/demodata.yml', dag=dag ) # A dummy operator to simulate data publication when the Soda SQL Scan task is successful publish_data_op = DummyOperator( task_id='publish_data' ) ingest_data_op >> soda_sql_scan_op >> publish_data_op ``` ## Prefect using a custom Task Create a custom Prefect Task to run Soda SQL scans programmatically. 1. Install Soda SQL and Prefect in your environment. 2. Make sure that Soda SQL is available in the environment in whic your Prefect flow runs. For example, if you use Docker as the storage backend, you must provide Soda SQL as a dependency. Refer to [Prefect documentation](https://docs.prefect.io/core/concepts/flows.html). 3. Define a custom Prefect Task to wrap Soda SQL and use it in your Prefect Flow. Refer to the following basic implementation of a SodaSQLScan Task. ```python from sodasql.scan.scan_builder import ScanBuilder from prefect import Task from prefect.utilities.tasks import defaults_from_attrs from typing import Dict, Union class SodaSQLScan(Task): \"\"\" SodaSQLScan \"\"\" def __init__( self, scan_def: Union[Dict, str] = None, warehouse_def: Union[Dict, str] = None, **kwargs ): \"\"\" Args: scan_def: Soda SQL scan file path or dictionary warehouse_def: Soda SQL warehouse file path or dictionary **kwargs: \"\"\" self.scan_builder = ScanBuilder() self._set_scan_def(scan_def=scan_def) self._set_warehouse_def(warehouse_def=warehouse_def) super().__init__(**kwargs) def _set_scan_def(self, scan_def: Union[Dict, str]) -> None: \"\"\" Args: scan_def: Soda SQL scan file path or dictionary Returns: \"\"\" self.scan_def = scan_def if isinstance(scan_def, str): self.scan_builder.scan_yml_file = scan_def elif isinstance(scan_def, dict): self.scan_builder.scan_yml_dict = scan_def def _set_warehouse_def(self, warehouse_def: Union[Dict, str]) -> None: \"\"\" Args: warehouse_def: Soda SQL warehouse file path or dictionary Returns: \"\"\" self.warehouse_def = warehouse_def if isinstance(warehouse_def, str): self.scan_builder.warehouse_yml_file = warehouse_def elif isinstance(warehouse_def, dict): self.scan_builder.warehouse_yml_dict = warehouse_def @defaults_from_attrs(\"scan_def\", \"warehouse_def\") def run(self, scan_def: Union[Dict, str], warehouse_def: Union[Dict, str]) -> Dict: \"\"\" Args: scan_def: Soda SQL scan file path or dictionary warehouse_def: Soda SQL warehouse file path or dictionary Returns: Soda SQL scan results as JSON object \"\"\" if scan_def is None: raise ValueError( \"Scan definition cannot be None. \\ Please provide either a path to a scan definition file or a scan definition dictionary\" ) if warehouse_def is None: raise ValueError( \"Warehouse definition cannot be None. \\ Please provide either a path to a warehouse definition file or a warehouse definition dictionary\" ) scan = self.scan_builder.build() return scan.execute().to_json() ``` In your Prefect Flow, call the Soda SQL Task as per the following example. ```python import SodaSQLScan from prefect import Flow dq_task = SodaSQLScan( scan_def=\"/path/to/scan.yml\", warehouse_def=\"/path/to/warehouse.yml\" ) with Flow(name=\"DQ Sample\") as f: f.add_task(dq_task) # run Flow locally, useful only for debugging # f.run() # Register flow to Prefect Server/Cloud f.register() ``` ## Go further - If you want to write integration instructions for your favorite data orchestration tool, please contribute to our open-source docs! [Post a note on GitHub](https://github.com/sodadata/soda-sql/discussions) to let us know your plans. - Need help? [Post your questions on GitHub](https://github.com/sodadata/soda-sql/discussions) or [join our Slack community](https://join.slack.com/t/soda-community/shared_invite/zt-m77gajo1-nXJF7JtbbRht2zwaiLb9pg) - Learn how to configure [programmatic Soda SQL scans]({% link documentation/programmatic_scan.md %}). ",
    "url": "/soda-sql/documentation/orchestrate_scans.html",
    "relUrl": "/documentation/orchestrate_scans.html"
  },"14": {
    "doc": "Configure programmatic scans",
    "title": "Configure programmatic scans",
    "content": "# Configure programmatic scans To automate the search for \"bad\" data, you can use the **Soda SQL Python library** to programmatically execute [scans]({% link documentation/glossary.md %}#scan). Based on a set of conditions or a specific event schedule, you can instruct Soda SQL to automatically scan a [warehouse]({% link documentation/glossary.md %}#warehouse) [table]({% link documentation/glossary.md %}#table) for “bad” data. For example, you may wish to scan your data at several points along your data pipeline, perhaps when new data enters a warehouse, after it is transformed, and before it is exported to another warehouse. Execute a programmatic scan based on Soda SQL's default directory structure: ```python scan_builder = ScanBuilder() scan_builder.scan_yml_file = 'tables/my_table.yml' # scan_builder automatically finds the warehouse.yml in the parent directory of the scan YAML file # scan_builder.warehouse_yml_file = '../warehouse.yml' scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_test_failures(): print('Scan has test failures, stop the pipeline') ``` Execute a programmatic scan using dicts: ```python scan_builder = ScanBuilder() scan_builder.warehouse_yml_dict = { 'name': 'my_warehouse_name', 'connection': { 'type': 'snowflake', ... } } scan_builder.scan_yml_dict = { ... } scan = scan_builder.build() scan_result = scan.execute() if scan_result.has_test_failures(): print('Scan has test failures, stop the pipeline') ``` ## Go further - Learn more about [How Soda SQL works]({% link documentation/concepts.md %}). - Learn more about [Warehouse]({% link documentation/warehouse.md %}) and [Scan]({% link documentation/scan.md %}) YAML files. - Learn how to integrate Soda SQL with a [data orchestration tool]({% link documentation/orchestrate_scans.md %}). - Need help? [Post your questions on GitHub](https://github.com/sodadata/soda-sql/discussions) or [join our Slack community](https://join.slack.com/t/soda-community/shared_invite/zt-m77gajo1-nXJF7JtbbRht2zwaiLb9pg) ",
    "url": "/soda-sql/documentation/programmatic_scan.html",
    "relUrl": "/documentation/programmatic_scan.html"
  },"15": {
    "doc": "Scan YAML",
    "title": "Scan YAML",
    "content": "# Scan YAML A **scan** is a Soda SQL CLI command that uses SQL queries to extract information about data in a database table. Instead of laboriously accessing your database and then manually defining SQL queries to analyze the data in tables, you can use a much simpler Soda SQL scan. First, you configure scan metrics and tests in a **scan YAML** file, then Soda SQL uses the input from that file to prepare, then run SQL queries against your data. **[Create a scan YAML file](#create-a-scan-yaml-file) [Anatomy of the scan YAML file](#anatomy-of-the-scan-yaml-file) [Scan YAML configuration keys](#scan-yaml-configuration-keys) [Run a scan](#run-a-scan) [Scan output](#scan-output)** ## Create a scan YAML file You need to create a **scan YAML** file for every table in your [warehouse]({% link documentation/glossary.md %}#warehouse) that you want to scan. If you have 20 tables in your warehouse, you need 20 YAML files, each corresponding to a single table. You can create scan YAML files yourself, but the CLI command `soda analyze` sifts through the contents of your warehouse and automatically prepares a scan YAML file for each table. Soda SQL puts the YAML files in a `/tables` directory in your [warehouse directory]({% link documentation/glossary.md %}#warehouse-directory). In your command-line interface, navigate to the directory that contains your `warehouse.yml` file, then execute the following: Command: ```shell $ soda analyze ``` Output: ```shell | Analyzing warehouse.yml ... | Querying warehouse for tables | Creating tables directory tables | Executing SQL query: SELECT table_name FROM information_schema.tables WHERE lower(table_schema)='public' | SQL took 0:00:00.008511 | Executing SQL query: SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE lower(table_name) = 'demodata' AND table_catalog = 'sodasql' AND table_schema = 'public' | SQL took 0:00:00.013018 | Executing SQL query: ... | SQL took 0:00:00.008593 | Creating tables/demodata.yml ... | Next run 'soda scan warehouse.yml tables/demodata.yml' to calculate measurements and run tests ``` In the above example, Soda SQL created a scan YAML file named `demodata.yml` and put it in the `/tables` directory. If you decide to create your own scan YAML files manually, best practice dictates that you name the YAML file using the same name as the table in your database. ## Anatomy of the scan YAML file When it creates your scan YAML file, Soda SQL pre-populates it with the `test` and `metric` configurations it deems useful based on the data in the table it analyzed. You can keep those configurations intact and use them to run your scans, or you can adjust or add to them to fine-tune the tests Soda SQL runs on your data. The following describes the contents of a scan YAML file that Soda SQL created and pre-populated. ![scan-anatomy](../assets/images/scan-anatomy.png){:height=\"440px\" width=\"440px\"} **1** - The value of **table_name** identifies a SQL table in your database. If you were writing a SQL query, it is the value you would supply for your `FROM` statement. **2** - A **metric** is a property of the data in your database. A **measurement** is the value for a metric that Soda SQL obtains during a scan. For example, in `row_count = 5`, `row_count` is the metric and `5` is the measurement. **3** - A **test** is a Python expression that, during a scan, checks metrics to see if they match the parameters defined for a measurement. As a result of a scan, a test either passes or fails. For example, the test `row_count > 0` checks to see if the table has at least one row. If the test passes, it means the table has at least one row; if the test fails, it means the table has no rows, which means that the table is empty. Tests in this part of the YAML file apply to all columns in the table. A single Soda SQL scan can run many tests on the contents of the whole table. **4** - A **column** identifies a SQL column in your table. Use column names to configure tests against individual columns in the table. A single Soda SQL scan can run many tests in many columns. **5** - **`id`** and **`feepct`** are column names that identify specific columns in this table. **6** - The value of the **column configuration key** `valid_format` identifies the only form of data in the column that Soda SQL recognizes as valid during a scan. In this case, any row in the `id` column that contains data that is in UUID format (universally unique identifier) is valid; anything else is invalid. **7** - Same as above, except the tests in the `column` section of the YAML file run only against the contents of the single, identified column. In this case, the test `invalid_percentage == 0` checks to see if all rows in the `id` column contain data in a valid format. If the test passes, it means that 0% of the rows contain data that is invalid; if the test fails, it means that more than 0% of the rows contain invalid data, which is data that is in non-UUID format. ## Scan YAML configuration keys The table below describes all of the top level configuration keys you can use to customize your scan. | Key | Description | Required | ----------- | ----------- | -------- | `columns` | The section of the scan YAML file in which you define tests and metrics that apply to individual columns. See [Metrics]({% link documentation/sql_metrics.md %}#column-metrics) for configuration details.| optional | `filter` | A SQL expression that Soda SQL adds to the `WHERE` clause in the query. Use `filter` to pass variables, such as date, into a scan. Uses [Jinja](https://jinja.palletsprojects.com/en/2.11.x/) as the templating language. See [Apply filters]({% link documentation/filtering.md %}) for configuration details.| optional | `frequent_values_limit` | Defines the maximum number of elements for the `maxs` metric. Default value is `5`.| optional | `metrics` | A list of all the default metrics that you can use to configure a scan. This list includes both table and column metrics. See [Metrics]({% link documentation/sql_metrics.md %}) for configuration details.| optional | `mins_maxs_limit` | Defines the maximum number of elements for the `mins` metric. Default value is `5`.| optional | `sample_method` | Defines the sample method Soda SQL uses when you specify a `sample_percentage`. For Snowflake, the values available for this key are: `BERNOULLI`, `ROW`, `SYSTEM`, and `BLOCK`. | required, if `sample_percentage` is specified | `sample_percentage` | Defines a limit to the number of rows in a table that Soda SQL scans during a scan. (Tested on Postgres only.) | optional| `sql_metrics` | The section of the scan YAML file in which you define custom sql queries to run during a scan. You can apply `sql_metrics` to all data in the table, or data in individual columns. See [Metrics]({% link documentation/sql_metrics.md %}#sql-metrics) for configuration details.| optional | `table_name` | Identifies a SQL table in your database. | required | ## Run a scan {% include run-a-scan.md %} When Soda SQL runs a scan, it performs the following actions: - fetches column metadata (column name, type, and nullable) - executes a single aggregation query that computes aggregate metrics for multiple columns, such as `missing`, `min`, or `max` - for each column, executes: - a query for `distinct_count`, `unique_count`, and `valid_count` - a query for `mins` (list of smallest values) - a query for `maxs` (list of greatest values) - a query for `frequent_values` - a query for `histograms` To allow some databases, such as Snowflake, to cache scan results, the column queries use the same Column Table Expression (CTE). This practice aims to improve overall scan performance. ## Scan output {% include scan-output.md %} ## Use the scan output Optionally, if you have a Soda Cloud account and you have [connected Soda SQL]({% link documentation/connect_to_cloud.md %}) to your account, Soda SQL automatically pushes the scan output to Soda Cloud. You can log in to view the Monitor Results; each row in the Monitor Results table represents a test, and the icon indicates the result of the latest scan, pass or fail. Optionally, you can insert the output of Soda SQL scans into your data orchestration tool such as Dagster, or Apache Airflow. In your orchestration tool, you can use Soda SQL scan results to block the data pipeline if it encounters bad data, or to run in parallel to surface issues with your data. Learn more about [orchestrating scans]({% link documentation/orchestrate_scans.md %}). ## Go further * Learn more about the [warehouse YAML]({% link documentation/warehouse.md %}) file. * Learn how to configure [metrics]({% link documentation/sql_metrics.md %}) in your YAML files. * Learn more about configuring [tests]({% link documentation/tests.md %}). * Learn how to apply [filters]({% link documentation/filtering.md %}) to your scan. ",
    "url": "/soda-sql/documentation/scan.html",
    "relUrl": "/documentation/scan.html"
  },"16": {
    "doc": "Soda Cloud architecture",
    "title": "Soda Cloud architecture",
    "content": "# Soda Cloud architecture ![scan-anatomy](../assets/images/soda-cloud-arch.png){:height=\"550px\" width=\"550px\"} **[Soda Cloud]({% link documentation/glossary.md %}#soda-cloud)** and **[Soda SQL]({% link documentation/glossary.md %}#soda-sql)** work together to help you monitor your data and alert you when there is a data quality issue. Installed in your environment, you use the Soda SQL command-line tool to [scan]({% link documentation/glossary.md %}#scan) data in your [warehouses]({% link documentation/glossary.md %}#warehouse). Soda SQL uses a secure API to connect to Soda Cloud. When it completes a scan, it pushes the scan results to your Soda Cloud account where you can log in and examine the details in the web application. Notably, Soda SQL only ever pushes *metadata* to Soda Cloud; all your data stays inside your private network. When you create a [monitor]({% link documentation/glossary.md %}#monitor) in Soda Cloud's web application, Soda SQL uses the monitor settings to add new [tests]({% link documentation/glossary.md %}#test) when it runs a scan on data in a specific warehouse. A monitor is essentially a way to create Soda SQL tests using the web application instead of adjusting [scan YAML file]({% link documentation/glossary.md %}#scan-yaml) contents directly in your Soda project directory. Beyond creating them, you can use monitors to track data quality over time. Soda Cloud stores your scan results and prepares charts that represent the volume of failed tests in each scan. These visualizations of your scan results enable you to see where your data quality is improving or deteriorating over time. ## Go further * Create a Soda Cloud account at [cloud.soda.io](https://cloud.soda.io/signup). * [Connect Soda SQL to Soda Cloud]({% link documentation/connect_to_cloud.md %}). * Learn more about [How Soda SQL works]({% link documentation/concepts.md %}). * Learn more about [Soda SQL scans]({% link documentation/scan.md %}). * Learn more about [Soda SQL tests]({% link documentation/tests.md %}) and [Soda Cloud monitors and alerts]({% link documentation/monitors.md %}). ",
    "url": "/soda-sql/documentation/soda-cloud-architecture.html",
    "relUrl": "/documentation/soda-cloud-architecture.html"
  },"17": {
    "doc": "Metrics",
    "title": "Metrics",
    "content": "# Metrics A **metric** is a property of the data in your database. A **measurement** is the value for a metric that Soda SQL obtains during a scan. For example, in `row_count = 5`, `row_count` is the metric and `5` is the measurement. The following sections detail the configuration for metrics you can customize in your [scan YAML file]({% link documentation/scan.md %}). **[Table metrics](#table-metrics) [Column metrics](#column-metrics) [SQL metrics](#sql-metrics)** ## Table metrics Use **table metrics** to define tests in your scan YAML file that apply to all data in the table during a scan. ![table-metrics](../assets/images/table-metrics.png){:height=\"440px\" width=\"440px\"} | Table metric | Description | ---------- | ---------------- | `row_count` | The total number of rows in a table. | `schema` | Identifies that the table is part of a schema. | #### Example tests using a table metric ```yaml tests: - row_count > 0 ``` Checks to see if the table has more than one row. The test passes if the table contains rows. ```yaml tests: - row_count =5 ``` Checks to see if the table has exactly five rows. The test fails if the table contains more or fewer than five rows. ## Column metrics Use **column metrics** to define tests in your scan YAML file that apply to specific columns in a table during a scan. Where a column metric references a valid or invalid value, or a limit, use the metric in conjunction with a **column configuration**. A Soda SQL scan uses the value of a column configuration key to determine if it should pass or fail a test. See [example](#example-test-using-a-column-metric) below. ![column-metrics](../assets/images/column-metrics.png){:height=\"440px\" width=\"440px\"} | Column metric | Description | Use with column config key(s) | ---------- | ---------------- | ------------------------------ | `avg` | The average of the values in a numeric column. | - | `avg_length` | The average length of a string. | - | `distinct` | The distinct contents in rows in a column. | - | `duplicate_count` | The number of rows that contain duplicated content. | - | `frequent_values` | The number of rows that contain content that most frequently occurs in the column. | - | `histogram` | A histogram calculated on the content of the column. | - | `invalid_count` | The total number of rows that contain invalid content. | `valid_format` `valid_regex` | `invalid_percentage` | The total percentage of rows that contain invalid content. | `valid_format` `valid_regex` | `max` | The greatest value in a numeric column. | - | `max_length` | The maximum length of a string. | `valid_max_length` | `maxs` | The number of rows that qualify as maximum. | `valid_max` | `min` | The smallest value in a numeric column. | - | `min_length` | The minimum length of a string. | `valid_min_length` | `mins` | The number of rows that qualify as minimum. | `valid_min` | `missing_count` | The total number of rows that are missing specific content. | `missing_values` `missing_regex`| `missing_percentage` | The total percentage of rows that are missing specific content. | `missing_values` `missing_regex` | `stddev` | The standard deviation of a numeric column. | - | `sum` | The sum of the values in a numeric column. | - | `unique_count` | The number of rows in which the content appears only once in the column. | - | `uniqueness` | A ratio that produces a number between 0 and 100 that indicates how unique a column is. 0 indicates that all the values are the same; 100 indicates that all the values in the the column are unique. | - | `valid_count` | The total number of rows that contain valid content. | `valid_format` `valid_regex` | `valid_percentage` | The total percentage of rows that contain valid content. | `valid_format` `valid_regex` | `values_count` | The total number of rows that contain content included in a list of valid values. | `valid_values` `valid_regex` | `values_percentage` | The total percentage of rows that contain content included in a list of valid values. | `valid_values` `valid_regex` | `variance` | The variance of a numerical column. | - | Column configuration key | Description | Values | ------------------------- | ------------ | ------ | `metric_groups` | Specifies pre-defined groups of metrics that Soda SQL computes for this column. See [Metric groups and dependencies](#metric-groups-and-dependencies) for details.| `duplicates` `length` `missing` `profiling` `statistics` `validity` | `metrics` | Specifies extra metrics that Soda SQL computes for this column. | - | `missing_format` | Specifies missing values such as whitespace or empty strings.| | `missing_regex` | Use regex expressions to specify your own custom missing values.| regex | `missing_values` | Specifies the values that Soda SQL is to consider missing in list format.| integers in list | `tests` | A section that contains the tests that Soda SQL runs on a column during a scan.| - | `valid_format` | Specifies a named valid text format.| See Valid_format value table below. | `valid_max` | Specifies a maximum value for valid values. | integer | `valid_max_length` | Specifies a maximum string length for valid values. | integer | `valid_min` | Specifies a minimum value for valid values. | integer | `valid_min_length` | Specifies a minimum string length for valid values. | integer | `valid_regex` | Use regex expressions to specify your own custom valid values. | regex | `valid_values` | Specifies several valid values in list format. | integers in list | > EXPERIMENTAL: The valid_format values are experimental and may change in the future. | `valid_format` value | Format | ----- | ------ | `number_whole` | Number is whole. | `number_decimal_point` | Number uses `.` as decimal indicator.| `number_decimal_comma` | Number uses `,` as decimal indicator.| `number_percentage` | Number is a percentage. | `number_money_usd` | Number matches US dollar currency pattern. | `number_money_eur` | Number matches Euro currency pattern. | `number_money_gbp` | Number matches British pound currency pattern. | `number_money_rmb` | Number matches Renminbi yuan currency pattern. | `number_money_chf` | Number matches Swiss franc currency pattern. | `number_money` | Format matches any of the `number_money_` patterns.| `date_eu` | dd/mm/yyyy | `date_us` | mm/dd/yyyy | `date_inverse` | yyyy/mm/dd | `time` | | `date_iso_8601` | yyyy-mm-dd | `uuid` | universally unique identifier | `ip_address` | Four whole numbers separated by `.` | `email` | email address | `phone_number` | | `credit_card_number` | Four four-digit numbers separated by spaces. | #### Example tests using a column metric ```yaml columns: id: valid_format: uuid tests: - invalid_percentage == 0 feepct: valid_format: number_percentage tests: - invalid_percentage == 0 ``` `invalid_percentage == 0` in column `id` with column configuration `valid_format: uuid` checks the rows in the column named `id` for values that match a uuid (universally unique identifier) format. If the test passes, it means that 0% of the rows contain data that is invalid; if the test fails, it means that more than 0% of the rows contain invalid data, which is data that is in non-UUID format. ## Metric groups and dependencies Out of the box, Soda SQL includes a **metric groups** configuration key. Define this configuration key in your scan YAML file so that when you use one of the group's metrics in a test, Soda SQL automatically runs the test against all the metrics in its group. In the example below, a Soda SQL scan runs a test on the contents of the `commission` column to look for values that are not in percentage format. Because the YAML file also defined `metric_group: validity`, the scan also tests all other metrics in the `validity` group. Refer to table, below. ```yaml columns: commission: metric_group: validity valid_format: number_percentage tests: – invalid_count == 0 ``` | `metric_groups` value | Metrics the scan includes | ------------------- | ----------------------- | `all` | all column metrics | `missing` | `missing_count`, `missing_percentage`, `values_count`, `values_percentage`. | `validity` | `valid_count`, `valid_percentage`, `invalid_count`, `invalide_percentage` | `duplicates` | `distinct`, `unique_count`, `uniqueness`, `duplicate_count` | `length` | `min_length`, `max_length`, `avg_length` | `profiling` | `maxs`, `mins`, `frequent_values`, `histogram` | `statistics` | `min`, `max`, `avg sum`, `variance`, `stddev` | By default, there exist **dependencies** between some metrics. If Soda SQL scans a metric which has dependencies, it includes all the dependent metrics in the scan as well. | If you use... | ...the scan includes: | ------ | ------------ | `valid_count` | `missing_count` | `valid_percentage` | `missing_percentage` | `invalid_count` | `values_count` | `invalid_percentage`| `values_percentage`| `missing_count` `missing_percentage` `values_count` `values_percentage` | `row_count` | `histogram` | `min` `max` | ## SQL metrics If the default set of table and column metrics that Soda SQL offers do not quite give you the information you need from a scan, you can use **SQL metrics** to customize your queries. SQL metrics essentially enable you to add SQL queries to your scan YAML file so that Soda SQL runs them during a scan. #### Simple example In your scan YAML file, use the `sql_metrics` property as a table metric or a column metric. The following simple SQL metric example queries all content in the table to select a single numeric value. ```yaml table_name: mytable sql_metrics: - sql: | SELECT sum(volume) as total_volume_us FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' tests: - total_volume_us > 5000 ``` In the example, the computed value (the sum volume of all customer transaction in the United States) becomes a **field** named `total_volume_us`, which, in turn, becomes the name of the metric that you use to define the test Soda SQL that runs on your data. In this case, the test passes if the computed sum of all US transactions exceeds `5000`. Notice that by default, Soda SQL uses the name of the field as the name of the metric. If you do not want to specify field names inside your SQL queries, you can explicitly name the metrics outside the queries. See [SQL metric names](#sql-metric-names) below. #### Multiple example You can also compute multiple metric values in a single query, then combine them in your tests. ```yaml table_name: mytable sql_metrics: - sql: | SELECT sum(volume) as total_volume_us, min(volume) as min_volume_us, max(volume) as max_volume_us FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' tests: - total_volume_us > 5000 - min_volume_us > 20 - max_volume_us > 100 - max_volume_us - min_volume_us 5000 ``` ### SQL metric names If you do not want to specify field names inside your SQL queries, you can use the **`metric_names` property** to explicitly name the metrics outside the queries. This property contains a list of values which match the order of values in your `SELECT` statement. ```yaml table_name: mytable sql_metrics: - sql: | SELECT sum(volume), min(volume), max(volume) FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' metric_names: - total_volume_us - min_volume_us - max_volume_us tests: - total_volume_us > 5000 - min_volume_us > 20 - max_volume_us > 100 - max_volume_us - min_volume_us 5000 - min_volume > 20 - max_volume > 100 - max_volume - min_volume 5000 ``` ### SQL metrics using file reference Instead of including all your customized SQL queries in the SQL metrics in your scan YAML file, you can use **`sql_file`** to reference a relative file. ```yaml table_name: mytable sql_metrics: - sql_file: mytable_metric_us_volume.sql tests: - total_volume_us > 5000 ``` In this case, the `mytable_metric_us_volume.sql` file contains the following SQL query. ```sql SELECT sum(volume) as total_volume_us FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' ``` ## Go further * Learn [How Soda SQL works]({% link documentation/concepts.md %}). * Learn more about the [Scan YAML]({% link documentation/scan.md %}) file. * Learn more about configuring [tests]({% link documentation/tests.md %}). * Learn how to apply [filters]({% link documentation/filtering.md %}) to your scan. ",
    "url": "/soda-sql/documentation/sql_metrics.html",
    "relUrl": "/documentation/sql_metrics.html"
  },"18": {
    "doc": "Tests",
    "title": "Tests",
    "content": "# Tests A **test** is a check that Soda SQL performs when it scans a table in your warehouse. Technically, it is a Python expression that, during a Soda SQL scan, checks metrics to see if they match the parameters defined for a measurement. A single Soda SQL scan runs against a single table in your database, but each scan can run multiple tests. As a result of a scan, each test either passes or fails. When a test fails, it means that a property of the data in your table did not match the test parameters you defined. In other words, any test that returns `true` during a Soda SQL scan passes; any test that returns `false`, fails. The **scan results** in the command-line interface (CLI) include an exit code which is an indicator of the test results: `0` means all tests passed; a non-zero value means one or more tests have failed. See [Scan output]({% link documentation/scan.md %}#scan-output) for details. ## Define tests You define your tests in your [scan YAML file]({% link documentation/scan.md %}) which is associated with a specific table in your database. You can write tests using **default metrics** that Soda SQL applies to an entire table (table metrics), or to individual columns you identify (column metrics). You can also write tests using **custom metrics** (SQL metrics) that you can apply to an entire table or individual columns. To learn more, see [Metrics]({% link documentation/sql_metrics.md %}). Regardless of where it applies, each test is comprised of three parts: - a metric (a property of the data in your database) - a comparison operator - a value For example: ```yaml tests: - row_count > 0 ``` However, where a test must determine whether or not data is valid, you must add a fourth element, a **column configuration key** to define what qualifies as valid. In the scan YAML file, you define a column configuration key before the test that will use the definition of \"valid\". In the example below, the user defined the `valid_format` as `date_eu` or dd/mm/yyyy format. The metric `invalid_percentage` refers to the `valid_format` configuration key to determine if the data in the column is valid. To see a list of all available column configuration keys, see [Column Metrics]({% link documentation/sql_metrics.md %}#column-metrics). ```yaml columns: start_date: valid_format: date_eu tests: - invalid_percentage #### Example tests using default metrics Reference the table below which corresponds to the following example scan YAML file. ```yaml table_name: demodata metrics: - row_count - missing_count - missing_percentage - values_count - values_percentage - valid_count - valid_percentage - invalid_count - invalid_percentage - ... tests: - row_count > 0 columns: id: valid_format: uuid tests: - invalid_percentage == 0 feepct: valid_format: number_percentage tests: - invalid_percentage == 0 ``` | Default metric | Comparison operator | Value | Applies to | Test | ------ | ------------------- | ----- | ---------- | ---- | `row_count` | `>` | `0` | whole table | Checks to see if the table has at least one row. If the test fails, it means the table has no rows, which means that the table is empty.| `invalid_percentage` | `==` | `0` | `id` column | Checks to see if all rows in the id column contain data in a valid format. If the test fails, it means that more than 0% of the rows contain invalid data, which is data that is in non-UUID format.| `invalid_percentage` | `==` | `0` | `feepct` column | Checks to see if all rows in the `feepct` column contain data in a valid format. If the test fails, it means that more than 0% of the rows contain invalid data, which is data that is not a numerical percentage.| #### Example tests using custom metrics If the default set of table and column metrics that Soda SQL offers do not quite give you the information you need from a scan, you can use **SQL metrics** to customize your queries. SQL metrics essentially enable you to add SQL queries to your scan YAML file so that Soda SQL runs them during a scan. See [SQL metrics]({% link documentation/sql_metrics.md %}#sql-metrics) Reference the table below which corresponds to the following example scan YAML file. ```yaml table_name: yourtable sql_metrics: - sql: | SELECT sum(volume) as total_volume_us FROM CUSTOMER_TRANSACTIONS WHERE country = 'US' tests: - total_volume_us > 5000 ``` | Custom metric | Comparison operator | Value | Applies to | Test | ------------- | ------------------- | ----- | ---------- | ---- | `total_volume_us` | `>` | `5000`| whole table | Checks to see if the sum of all customer transactions in the United States exceeds `5000`. If the test fails, it means that the total volume of transactions is less than `5000`. ## Define test names Soda SQL can run both anonymous or named tests. Named tests are useful if you intend to push Soda SQL scan results to your Soda Cloud account where you can update a test and retain its test history. Example of an anonymous test ```yaml tests: - total_volume_us > 5000 ``` Examples of named tests ```yaml tests: volume_test_max: total_volume_us > 3000 volume_test_min: total_volume_us < 5000 ``` ## Go further * Learn more about [Metrics]({% link documentation/sql_metrics.md %}). * Learn about [How Soda works]({% link documentation/concepts.md %}). ",
    "url": "/soda-sql/documentation/tests.html",
    "relUrl": "/documentation/tests.html"
  },"19": {
    "doc": "Troubleshoot",
    "title": "Troubleshoot",
    "content": "# Troubleshoot ## Install and upgrade {% include troubleshoot-install.md %} ## Tests and metrics {% include troubleshoot-tests.md %} ## Warehouse connections {% include troubleshoot-warehouses.md %} ",
    "url": "/soda-sql/documentation/troubleshoot.html",
    "relUrl": "/documentation/troubleshoot.html"
  },"20": {
    "doc": "Warehouse YAML",
    "title": "Warehouse YAML",
    "content": "# Warehouse YAML A **warehouse** represents a SQL engine or database such as Snowflake, AWS Redshift, or PostgreSQL. You use a **warehouse YAML** file to configure connection details for Soda SQL to access your warehouse. ## Create a warehouse YAML file You need to create a warehouse YAML file for every warehouse to which you want to connect. You can create warehouse YAML files manually, but the CLI command `soda create` automatically prepares a warehouse YAML file and an env_vars YAML file for you. (Use the env-vars YAML to securely store warehouse login credentials. See [Env_vars YAML](#env_vars-yaml) below.) When you execute the `soda create` command, you include options that instruct Soda SQL in the creation of the file, and you indicate the type of warehouse, a specification Soda SQL requires. Use `soda create --help` for a list of all available options. The example below provides the following details: * option `-d` provides the name of the database * option `-u` provides the username to log in to the database * option `-w` provides the name of the warehouse directory * requirement `postgres` indicates the type of database Command: ```shell $ soda create -d sodasql -u sodasql -w soda_sql_tutorial postgres ``` Output: ```shell | Soda CLI version 2.x.xx | Creating warehouse YAML file warehouse.yml ... | Creating /Users/Me/.soda/env_vars.yml with example env vars in section soda_sql_tutorial | Review warehouse.yml by running command | cat warehouse.yml | Review section soda_sql_tutorial in ~/.soda/env_vars.yml by running command | cat ~/.soda/env_vars.yml | Then run the soda analyze command ``` In the above example, Soda SQL created a warehouse YAML file and put it in a **warehouse directory** which is the top directory in your Soda SQL project directory structure. (It puts the env_vars YAML file in your local user home directory.) ## Anatomy of the warehouse YAML file When it creates your warehouse YAML file, Soda SQL pre-populates it with the options details you provided. The following is an example of a warehouse YAML file that Soda SQL created and pre-populated. ```yaml name: soda_sql_tutorial connection: type: postgres host: localhost username: env_var(POSTGRES_USERNAME) password: env_var(POSTGRES_PASSWORD) database: sodasql schema: public ``` Notice that even though the command provided a value for `username`, Soda SQL automatically used `env_var(POSTGRES_USERNAME)` instead. By default, Soda SQL stores database login credentials in an env_vars YAML file so that this sensitive information stays locally stored. See [Env_vars YAML](#env_vars-yaml) below for details. Each type of warehouse requires different configuration parameters. Refer to [Set warehouse configurations]({% link documentation/warehouse_types.md %}) for details that correspond to the type of database you are using. ## Env_vars YAML To keep your warehouse YAML file free of login credentials, Soda SQL references environment variables. When it creates a new warehouse YAML file, Soda SQL also creates an **env_vars YAML** file to store your database username and password values. Soda SQL does not overwrite or remove and existing environment variables, it only adds new. When it [runs a scan]({% link documentation/scan.md %}#run-a-scan), Soda SQL loads environment variables from your local user home directory where it stored your env_vars YAML file. Use the command `cat ~/.soda/env_vars.yml` to review the contents of your env_vars YAML file. Open the file from your local user home directory to input the values for your database credentials. ```yaml soda_sql_tutorial: POSTGRES_USERNAME: myexampleusername POSTGRES_PASSWORD: myexamplepassword some_other_soda_project: SNOWFLAKE_USERNAME: someotherexampleusername SNOWFLAKE_PASSWORD: someotherexamplepassword ``` ## Go further * Set the [warehouse configuration parameters]({% link documentation/warehouse_types.md %}) for your type of database. * Learn more about [How Soda SQL works]({% link documentation/concepts.md %}). * Learn more about the [scan YAML]({% link documentation/scan.md %}) file. ",
    "url": "/soda-sql/documentation/warehouse.html",
    "relUrl": "/documentation/warehouse.html"
  },"21": {
    "doc": "Set warehouse configurations",
    "title": "Set warehouse configurations",
    "content": "# Set warehouse configurations Soda SQL needs connection details in order to access your [warehouse]({% link documentation/glossary.md %}#warehouse) to scan your data. Each type of warehouse uses different configuration parameters. To set the warehouse configurations in your [warehouse YAML]({% link documentation/warehouse.md %}), use the following example configurations that correspond to each database type that Soda SQL supports. [Apache Hive](#apache-hive) [AWS Athena](#aws-athena) [AWS Redshift](#aws-redshift) [Google Cloud Platform BigQuery](#gcp-bigquery) [Microsoft SQL Server](#microsoft-sql-server) [PostgreSQL](#postgresql) [Snowflake](#snowflake) ## Apache Hive ```yaml name: my_hive_project connection: type: hive host: localhost port: 10000 username: env_var(HIVE_USERNAME) password: env_var(HIVE_PASSWORD) database: default configuration: hive.execution.engine: mr mapreduce.job.reduces: 2 ... ``` ## AWS Athena ```yaml name: my_athena_project connection: type: athena catalog: AwsDataCatalog database: sodalite_test access_key_id: env_var(AWS_ACCESS_KEY_ID) secret_access_key: env_var(AWS_SECRET_ACCESS_KEY) role_arn: an optional IAM role arn to be assumed region: eu-west-1 staging_dir: ... ``` | Property | Description | Required | -------- | ----------- | -------- | type | `snowflake` | Required | catalog | Optional (default is `AwsDataCatalog`) | database | Required | staging_dir | Required | access_key_id | Optional | secret_access_key | Optional | role_arn | Optional | region | Optional | ## AWS Redshift ```yaml name: my_redshift_project connection: type: redshift host: username: soda password: database: soda_agent_test schema: public access_key_id: env_var(AWS_ACCESS_KEY_ID) secret_access_key: env_var(AWS_SECRET_ACCESS_KEY) role_arn: an optional IAM role arn to be assumed region: eu-west-1 ... ``` ## GCP BigQuery ```yaml name: my_bigquery_project connection: type: bigquery # YOUR BIGQUERY SERVICE ACCOUNT INFO JSON FILE account_info_json: > { \"type\": \"service_account\", \"project_id\": \"...\", \"private_key_id\": \"...\", \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\", \"client_email\": \"user@project.iam.gserviceaccount.com\", \"client_id\": \"...\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://oauth2.googleapis.com/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/...\" } dataset: sodasql ... ``` ## Microsoft SQL Server Note: This connection is experimental. ```yaml name: my_sqlserver_project connection: type: sqlserver host: username: env_var(SQL_SERVER_USERNAME) password: env_var(SQL_SERVER_PASSWORD) database: master schema: dbo ``` ## PostgreSQL ```yaml name: my_postgres_project connection: type: postgres host: localhost username: sodasql password: sodasql database: sodasql schema: public ... ``` ## Snowflake ```yaml name: my_snowflake_project connection: type: snowflake username: env_var(SNOWFLAKE_USERNAME) password: env_var(SNOWFLAKE_PASSWORD) account: YOUR_SNOWFLAKE_ACCOUNT.eu-west-1 warehouse: YOUR_WAREHOUSE database: YOUR_DATABASE schema: PUBLIC ... ``` | Property | Description | Required | -------- | ----------- | -------- | type | `snowflake` | Required | username | Required | password | Required | account | Eg YOUR_SNOWFLAKE_ACCOUNT.eu-west-1 | Required | warehouse | Required | database | Required | schema | Required | ",
    "url": "/soda-sql/documentation/warehouse_types.html",
    "relUrl": "/documentation/warehouse_types.html"
  }
}
